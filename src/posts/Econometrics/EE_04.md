---
title: Econometrics - Week 4
category: Notes
---


# 多元线性回归模型

$
Y=\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{k} X_{k}+u,
$

其中 $ u $ 满足
$
E\left[u \mid X_{1}, \ldots, X_{k}\right]=0
$

- $ \beta_{0} $ ：截距，
- $ \beta_{1}, \ldots, \beta_{k} $ ：斜率参数
- $ u $ ：误差项或干扰项

## 样本模型

$
Y_{i}=\beta_{0}+\beta_{1} X_{i 1}+\cdots+\beta_{k} X_{i k}+u_{i},
$

其中 $ u_{i} $ 满足
$
E\left[u_{i} \mid X_{i}\right]=0 .
$

基本任务：构造参数 $ \beta_{0}, \beta_{1}, \ldots, \beta_{k} $ 的估计量。

### 方法1: 矩估计法

$ E\left[\begin{array}{c}Y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\cdots-\beta_{k} X_{i k} \\ X_{i 1}\left(Y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\cdots-\beta_{k} X_{i k}\right) \\ \cdots \\ X_{i k}\left(Y_{i}-\beta_{0}-\beta_{1} X_{i 1}-\cdots-\beta_{k} X_{i k}\right)\end{array}\right]=0 $



### 方法2: 最小二乘法

令 $ \beta=\left(\beta_{0}, \beta_{1}, \ldots, \beta_{k}\right) $ 。考虑最小化如下的目标函数：
$
S(\beta)=\sum_{i=1}^{n}\left[Y_{i}-\left(\beta_{0}-\beta_{1} X_{i 1}-\cdots-\beta_{k} X_{i k}\right)\right]^{2}
$

## 分布回归： Frisch-Waugh-Lovell定理

对任意 $ j=1,2, \ldots, k, \hat{\beta}_{j} $ 可以通过下面的两步法得到：
（1）以 $ X_{j} $ 做被解释变量，$ X_{m}, m \neq j $ 为解释变量做线性回归模型。记此回归模型的残差项为 $ \tilde{r}_{i j}, i=1, \ldots, n $ 。也就是说：$ \tilde{r}_{j}: X_{j} \sim \mathbf{X}_{-j} $
（2）以 $ Y $ 做被解释变量，$ \tilde{r}_{j} $ 为解释变量做简单线性回归模型 $ Y \sim \tilde{r}_{j} $ 。由此得到的 $ \tilde{r}_{j} $ 的系数的最小二乘估计等于 $ \hat{\beta}_{j} $ 。也就是说：
$
\hat{\beta}_{j}=\frac{\sum_{i=1}^{n}\left(\tilde{r}_{i j}-\tilde{\tilde{r}}_{j}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(\tilde{r}_{i j}-\overline{\tilde{r}}_{j}\right)^{2}}=\frac{\sum_{i=1}^{n} \tilde{r}_{i j} Y_{i}}{\sum_{i=1}^{n} \tilde{r}_{i j}^{2}}
$

### 另一种形式：

对任意 $ j=1,2, \ldots, k, \hat{\beta}_{j} $ 可以通过下面的两步法得到：
（1）以 $ X_{j} $ 做被解释变量，$ X_{m}, m \neq j $ 为解释变量做线性回归模型。记此回归模型的残差项为 $ \tilde{r}_{i j}, i=1, \ldots, n $ 。也就是说：$ \tilde{r}_{j}: X_{j} \sim \mathbf{X}_{-j} $ ，并记 $ R_{X_{j} \mathbf{X}_{-j}}^{2} $
（2）以 $ Y $ 做被解释变量，$ X_{m}, m \neq j $ 为解释变量做线性回归模型。记此回归模型的残差项为 $ \tilde{r_{i j}}, i=1, \ldots, n $ 。也就是说：$ \tilde{\tilde{r}}_{j}: Y \sim \mathbf{X}_{-j} $ ，并记 $ R_{Y \mathbf{X}_{-j}}^{2} $
（3）以 $ \tilde{r}_{j} $ 为被解释变量，$ \tilde{r}_{j} $ 为解释变量做简单线性回归模型 $ \tilde{\tilde{r}}_{j} \sim \tilde{r}_{j} $ 。由此得到的 $ \tilde{r}_{j} $ 的系数的最小二乘估计等于 $ \hat{\beta}_{j} $ 。也就是说：
$
\hat{\beta}_{j}=\frac{\sum_{i=1}^{n} \tilde{r}_{i j} \tilde{r}_{i j}}{\sum_{i=1}^{n} \tilde{r}_{i j}^{2}}
$

## 偏相关系数

给定一组变量 $ \mathbf{X}_{-j}=\left(X_{m}, m \neq j\right), Y $ 和 $ X_{j} $ 的偏相关系数等于 $ \tilde{r}_{j} $ 和 $ \tilde{r}_{j} $ 的相关系数。也就是说：

$ \rho_{Y X_{j} \cdot \mathbf{X}{-j}} = \text{Corr}(\tilde{\mathbf{e}}Y, \tilde{\mathbf{r}}j) = \frac{\sum{i=1}^{n} (\tilde{e}{iY} - \overline{\tilde{e}}Y)(\tilde{r}{ij} - \overline{\tilde{r}}j)}{\sqrt{\sum{i=1}^{n} (\tilde{e}{iY} - \overline{\tilde{e}}Y)^2} \sqrt{\sum{i=1}^{n} (\tilde{r}_{ij} - \overline{\tilde{r}}_j)^2}} $

从而我们有
$
\hat{\beta}_{j}=\rho_{Y X_{j}} \cdot \mathbf{X}_{-j} \sqrt{\frac{\sum_{i=1}^{n} \tilde{r}_{i j}^{2}}{\sum_{i=1}^{n} \tilde{r}_{i j}^{2}}}=\rho_{Y X_{j} \cdot \mathbf{X}_{-j}} \frac{s Y}{s X_{j}} \sqrt{\frac{1-R_{Y \mathbf{X}_{-j}}^{2}}{1-R_{X_{j} \mathbf{X}_{-j}}^{2}}}
$

易见，上式是简单线性模型时 $ \left(\mathbf{X}_{-j}\right. $ 为常数 $ ) $ 的公式 $ \hat{\beta}_{1}=r_{X Y} s_{Y} / s_{X} $ 的推广。

# 性质

定义 $ \hat{Y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} X_{i 1}+\cdots+\hat{\beta}_{k} X_{i k} $ 为 $ Y_{i} $ 的拟合值，定义 $ \hat{u}_{i}=Y_{i}-\hat{Y}_{i} $ 为残差。

性质1 所有残差之和为零：
$
\sum_{i=1}^{n} \hat{u}_{i}=0,
$

从而 $ \overline{\hat{Y}}=\bar{Y} $ 。
性质2 任一解释变量 $ X_{j} $ 与残差的样本协方差为零：
$
\sum_{i=1}^{n} X_{i j} \hat{u}_{i}=0, \forall j=1, \ldots, k .
$

性质3 拟合值 $ \hat{Y} $ 与残差的样本协方差为零：
$
\sum_{i=1}^{n} \hat{Y}_{i} \hat{u}_{i}=0
$

类似的，我们定义SST，SSE和SSR如下：
$
\begin{aligned}
S S T & =\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2} \\
S S E & =\sum_{i=1}^{n}\left(\hat{Y}_{i}-\bar{Y}\right)^{2} \\
S S R & =\sum_{i=1}^{n} \hat{u}_{i}^{2}
\end{aligned}
$

我们有：
$
S S T=S S E+S S R
$

我们定义 $ R^{2} $ 为
$
R^{2}=\frac{S S E}{S S T}=1-\frac{S S R}{S S T}
$

令 $ r_{Y} Y $ 为 $ \hat{Y} $ 与 $ Y $ 的相关系数，也就是：
$
r_{\hat{Y} Y}=\frac{\sum_{i=1}^{n}\left(\hat{Y}_{i}-\overline{\hat{Y}}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(\hat{Y}_{i}-\overline{\hat{Y}}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}
$

则我们有如下结果：
$
R^{2}=r_{\hat{Y} Y}^{2}
$

## 误差项的条件方差矩阵

误差项 $ \mathbf{u} $ 的条件方差矩阵 $ \operatorname{Var}(\mathbf{u} \mid \mathbf{X}) $ 的定义为
$
\begin{array}{l}
\operatorname{Var}(\mathbf{u} \mid \mathbf{X})=\left[\begin{array}{cccc}
\operatorname{Var}\left(u_{1} \mid \mathbf{X}\right) & \operatorname{Cov}\left(u_{1}, u_{2} \mid \mathbf{X}\right) & \ldots & \operatorname{Cov}\left(u_{1}, u_{n} \mid \mathbf{X}\right) \\
\vdots & \vdots & \vdots & \vdots \\
\operatorname{Cov}\left(u_{n}, u_{1} \mid \mathbf{X}\right) & \operatorname{Cov}\left(u_{n}, u_{2} \mid \mathbf{X}\right) & \ldots & \operatorname{Var}\left(u_{n} \mid \mathbf{X}\right)
\end{array}\right] \\
=\left[\begin{array}{cccc}
\operatorname{Var}\left(u_{1} \mid X_{1}\right) & \operatorname{Cov}\left(u_{1}, u_{2} \mid X_{1}, X_{2}\right) & \ldots & \operatorname{Cov}\left(u_{1}, u_{n} \mid X_{1}, X_{n}\right) \\
\vdots & \vdots & \vdots & \vdots \\
\operatorname{Cov}\left(u_{n}, u_{1} \mid X_{1}, X_{n}\right) & \operatorname{Cov}\left(u_{n}, u_{2} \mid X_{2}, X_{n}\right) & \ldots & \operatorname{Var}\left(u_{n} \mid X_{n}\right)
\end{array}\right]
\end{array}
$

## 同方差性

我们称回归模型具有同方差性，如果假设MLR．5成立，也就是有如下情况：
$
\operatorname{Var}(\mathbf{u} \mid \mathbf{X})=\left[\begin{array}{lll}
\sigma^{2} & & \\
& \ddots & \\
& & \sigma^{2}
\end{array}\right]=\sigma^{2} I .
$

也就是说，对于 $ i=1, \ldots, n $ ，有
$
\operatorname{Var}\left(u_{i} \mid \mathbf{X}\right)=\sigma^{2},
$

且对于 $ i \neq j $ ，有
$
\operatorname{Cov}\left(u_{i}, u_{j} \mid \mathbf{X}\right)=0 .
$

## 最小二乘估计的无偏性

在假设条件MLR．1－MLR．4下，对于任意的 $ j=0,1, \ldots, k $ ， $ \hat{\beta}_{j} $ 是 $ \beta_{j} $ 的无偏估计量：
$
E\left[\hat{\beta}_{j} \mid X_{1}, \ldots, X_{n}\right]=\beta_{j}
$

从而我们有
$
E\left[\hat{\beta}_{j}\right]=\beta_{j}
$

## 忽略有关变量的偏差

令 $ \hat{\beta}_{j}(j=0,1, \ldots, k) $ 为
$
Y \sim X_{1}+\cdots+X_{k}
$

回归得到的最小二乘估计量，令 $ \tilde{\beta}_{j}(j=0,1, \ldots, k-1) $ 为
$
Y \sim X_{1}+\cdots+X_{k-1}
$

回归得到的最小二乘估计量。令 $ \tilde{\delta}_{j}(j=1, \ldots, k-1) $ 为
$
X_{k} \sim X_{1}+\cdots+X_{k-1}
$

回归中 $ X_{j} $ 的系数的最小二乘估计。则对于任意 $ j=1, \ldots, k-1 $ ，有
$
\tilde{\beta}_{j}=\hat{\beta}_{j}+\hat{\beta}_{k} \tilde{\delta}_{j}
$

从而我们有
$
E\left[\tilde{\beta}_{j} \mid X_{1}, \ldots, X_{k}\right]=\beta_{j}+\beta_{k} \tilde{\delta}_{j}
$

## 控制变量

假设我们关心的是变量 $ X $ 对 $ Y $ 的影响，我们知道除了 $ X $ 之外，还有 $ Z_{1}, \ldots, Z_{p} $ 也会同时影响 $ Y $ 。这些 $ Z_{1}, \ldots, Z_{p} $ 通常叫做控制变量。
$
\begin{aligned}
Y & =\alpha+\beta X+\gamma_{1} Z_{1}+\cdots+\gamma_{p} Z_{p}+u, \\
E\left[u \mid X, Z_{1}, \ldots, Z_{p}\right] & =0 .
\end{aligned}
$

现在假设 $ X $ 与 $ Z_{1}, \ldots, Z_{p} $ 都不相关。






