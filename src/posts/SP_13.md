# Renewal Process 

Renewal Process: Let $ \{N(t), t \geq 0\} $ be a counting process.

If the sequence of nonnegative random variables $ \left\{X_{1}, X_{2}, \ldots\right\} $ is independent and identically distributed (i.i.d.), then the counting process $ \{N(t), t \geq 0\} $ is called a renewal process.

- $ N(t) $ represents the number of events that have occurred up to time $ t $.
- Let $ X_{n} $ denote the interarrival time between the ( $ n-1 $ )-th and $ n $-th events of this process ( $ n \geq 1 $ ). 

---

$S_0 = 0, S_n = \sum_{i=1}^n X_i$

# $N(t)$ and $S_n$

$ N(t)\ge n \Lrarr S_n \le t$

$N(t) = \max \{n:S_n\le t\}$

$S_{N(t)}\le t ,S_{N(t)+1}>t$

---

# Distribution of $N(t)$

$P\{N(t) = n\} = G_n(t) - G_{n+1}(t)$

Where: $X\sim G, G_n$ is convolution


---

$P\{N(t) = n\} = \int_0^t \bar G(t-y)f_{S_n}(y)dy$

Where: $\bar G = 1-G$

# Renewal Function

$m(t) = E[N(t)]$: Renewal Function 

# Limit Theorem 

- when $t\to\infty$, $\dfrac{N(t)}{t}\to\dfrac{1}{\mu}$

# Renewal Reward Process 

Consider a renewal process $ \{N(t), t \geq 0\} $ with inter-arrival times $ X_{n} $, where $ E\left[X_{n}\right]=E[X] $.

At each renewal $ n $, a reward $ R_{n} $ is received,
The process $ \{R(t), t \geq 0\} $ is called a renewal reward process.

- .$ R(t)=\sum_{n=1}^{N(t)} R_{n} $, representing the total reward earned up to time $ t $.

- $ R_{n} $ may depend on $ X_{n} $ or be accumulated during the process
- $ R_{n}(n \geq 1) $ are independent and identically distributed with $ E\left[R_{n}\right]=E[R] $.

## Proposition

If $ E[R]<\infty $ and $ E[X]<\infty $, then:

- $ \lim _{t \rightarrow \infty} \frac{R(t)}{t}=\frac{E[R]}{E[X]} $. w.p. 1
- $ \lim _{t \rightarrow \infty} \frac{E[R(t)]}{t}=\frac{E[R]}{E[X]} $.

## Elementary Renewal Theorem

As $t\to\infty$:

$ \frac{m(t)}{t} \rightarrow \dfrac{1}{\mu} $,

where $ \frac{1}{\mu} $ is interpreted as 0 when $ \mu=\infty $.

## Stopping Time 

A stopping time$ N $ for the stochastic process $ \left\{X_{n}, n \geq 1\right\} $ is a possibly infinite positive integer-valued random variable. 

- If for every $ n=1,2, \ldots $, the event $ \{N=n\} $ is entirely determined by $ X_{1}, \ldots, X_{n} $ 
  - (i.e., $ \{N=n\} $ is independent of $ X_{n+1}, X_{n+2}, \ldots $ ), 
- then $ N $ is called a stopping time for the sequence of independent random variables $ X_{1}, X_{2}, \ldots $.

## Wald Equation 

$ E\left[\sum_{n=1}^{N} X_{n}\right]=E[N] E[X] $

- Where $N$ is a stopping time of $X$

### Proof:

 $ \mathrm{E}\left[\sum_{n=1}^{N} X_{n}\right]=E\left[\sum_{n=1}^{\infty} X_{n} I_{n}\right]=\sum_{n=1}^{\infty} E\left[X_{n} I_{n}\right]= \sum_{n=1}^{\infty} E\left[X_{n}\right] E\left[I_{n}\right]=E[X] \sum_{n=1}^{\infty} E\left[I_{n}\right]=E[X] E\left[\sum_{n=1}^{\infty} I_{n}\right]=  E[X] E[N] $

 ## Proposition 

 $ E\left[X_{1}+\cdots+X_{N(t)+1}\right]=E[X] E[N(t)+1] $

 $ E\left[S_{N(t)+1}\right]=\mu[m(t)+1] $

 