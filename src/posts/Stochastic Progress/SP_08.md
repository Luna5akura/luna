---
title: Stochastic Progress - Week 8
category: Notes
---

# Exponential Distribution

$ f(x)=\left\{\begin{array}{cc}\lambda e^{-\lambda x}, & x \geq 0 \\ 0 & , x<0\end{array}\right. $

$ \mathrm{F}(x)=\int_{-\infty}^{x} f(y) d y=\left\{\begin{array}{cl}1-e^{-\lambda x} & , x \geq 0 \\ 0 & , x<0\end{array}\right. $

$ \begin{array}{l}P\{X \leq x\}=1-e^{-\lambda x} \\ P\{X>x\}=1-P\{X \leq x\}=e^{-\lambda x}\end{array} $



## Example:

Let $T$ as Exponential RV with parameter $\alpha $:

$ \int_{0}^{\infty} e^{-\alpha x} E[R(x)] d x=E\left[\int_{0}^{T} R(x) d x\right] $

Proof:

$ \begin{aligned} &E\left[\int_{0}^{\infty} R(x) I(x) d x\right] \\ 
& =\int_{0}^{\infty} E[R(x) I(x)] d x \\ & =\int_{0}^{\infty} E[R(x)] E[I(x)] d x \\ & =\int_{0}^{\infty} E[R(x)] P\{T \geq x\} d x \\ & =\int_{0}^{\infty} e^{-\alpha x} E[R(x)] d x\end{aligned} $

## Markov property

$ P\{X>s+t \mid X>t\}=P\{X>s\} $

Other form:

$ \dfrac{P\{X>s+t, X>t\}}{P\{X>t\}}=P\{X>s\} $

$ P\{X>s+t\}=P\{X>t\} P\{X>s\} $

### Proof

$ P(X>s+t \mid X>s)=\dfrac{P(X>s+t \cap X>s)}{P(X>s)} =\dfrac{e^{-\lambda(s+t)}}{e^{-\lambda s}}=e^{-\lambda t}=P(X>t)  $

Risk rate function: $ r(t)=\dfrac{f(t)}{1-F(t)} $

$ \begin{array}{c}P\{X \in(t, t+d t) \mid X>t\}=\dfrac{P\{X \in(t, t+d t), X>t\}}{P\{X>t\}} \\ =\dfrac{P\{X \in(t, t+d t)\}}{P\{X>t\}} \approx \dfrac{f(t) d t}{1-F(t)}=r(t) d t\end{array} $

$r(t) = \lambda$

$ r(t)=\dfrac{f(t)}{1-F(t)}=\dfrac{\dfrac{d}{d t} F(t)}{1-F(t)} $

Then can get $F(t)$

## Example x

$X_1,\dots,X_n$ are independent Exponential RVs with parameter $\lambda_i$.

$ \sum_{j=1}^{n} P_{j}=1 $, where $ P_{j}=P\{T=j\} $

Then for $X_T$:

$ \begin{array}{c}f(t)=\sum_{i=1}^{n} \lambda_{i} P_{i} e^{-\lambda_{i} t} \\ F(t)=1-\sum_{i=1}^{n} P_{i} e^{-\lambda_{i} t} \\ r(t)=\dfrac{\sum_{j=1}^{n} \lambda_{j} P_{j} e^{-\lambda_{j} t}}{\sum_{i=1}^{n} P_{i} e^{-\lambda_{i} t}}=\sum_{j=1}^{n} \lambda_{j} P\{T=j \mid X>t\} \\ \lim _{t \rightarrow \infty} r(t)=\min \lambda_{i}\end{array} $

## Propositions

1. $X_i$ are i.i.d. Exponential RVs with parameter $\lambda$, then $\sum_{i=1}^{n} X_i\sim Ga(n,\lambda)$ 

2. $ \min \left(X_{1}, \ldots, X_{n}\right) \sim \operatorname{Exp}\left(\sum_{i=1}^{n} \lambda_{i}\right) $

3. $ P\left\{X_{1}<X_{2}\right\}=\dfrac{\lambda_{1}}{\lambda_{1}+\lambda_{2}} $

4. $P(X_i = \min_j X_j) = \dfrac{\lambda_i}{\sum_{j=1}^{n} \lambda_j} $

5. $ P\left\{X_{i_{1}}<\cdots<X_{i_{n}} \mid \min _{\mathrm{i}} X_{i}>t\right\}  =P\left\{X_{i_{1}}<\cdots<X_{i_{n}}\right\}$

## Convolution

$ f_{S}(t)=f_{X_{1}+\cdots+X_{n}}(t)=\sum_{i=1}^{n} C_{i, n} \lambda_{i} e^{-\lambda_{i} t} $, where $ C_{i, n}=\prod_{j \neq i} \dfrac{\lambda_{j}}{\lambda_{j}-\lambda_{i}} $

$ r_{S}(t)=\dfrac{\sum_{i=1}^{n} C_{i, n} \lambda_{i} e^{-\lambda_{i} t}}{\sum_{i=1}^{n} C_{i, n} e^{-\lambda_{i} t}} $

$ \lim _{t \rightarrow \infty} r_{S}(t)=\min \left(\lambda_{1}, \ldots, \lambda_{n}\right)=\lambda^{*} $

## An extension

$ \mathrm{Y}=\sum_{i=1}^{N} X_{i} $ 

Then:


$
f_{Y}(t)=\sum_{n=1}^{m} P_{n} \sum_{i=1}^{n} C_{i, n} \lambda_{i} e^{-\lambda_{i} t},
$

# Counting Process

$\{N(t), t \geq 0\}$ is a counting process.

If:

1. $N(t) \ge 0$ 
2. $N(t)$ is integer
3. if $s < t$, then $N(s) \le N(t)$
4. for $s < t$, $N(t) - N(s)$ is the event counts in $(s, t]$

## independent increments

For any integer $ n $ and  $ 0 \leq t_{1}<t_{2}<\cdots<t_{n} $
$ >N(0), N\left(t_{1}\right)-N(0), N\left(t_{2}\right)-N\left(t_{1}\right), \ldots, N\left(t_{n}\right)-N\left(t_{n-1}\right) $ independent.

## Stationary increments

For any $ s>0 $ 和 $ 0 \leq t_{1}<t_{2} $ ， $ >N\left(t_{1}, t_{2}\right], N\left(t_{1}+s, t_{2}+s\right] $ i.d.

# Poission Process

## $o(h)$

$\lim_{h\to 0} \dfrac{f(h)}{h}=0$, then $o(h) := f(\cdot)$ 

## Definition

1. $N(0)=0$
2. $\{N(t), t \geq 0\}$ has Stationary increments and independent increments.
3. $P\{N(t+h)-N(t)=1\}=\lambda h+o(h) $
4. $P\{N(t+h)-N(t) \geq 2\}=o(h)$

## Proof 

$ P_{k}(t+h)=P_{k}(t)(1-\lambda h)+P_{k-1}(t) \lambda h+o(h) $.

$ \dfrac{d P_{k}(t)}{d t}=-\lambda P_{k}(t)+\lambda P_{k-1}(t) \quad(k \geq 1) $

$ \dfrac{d P_{0}(t)}{d t}=-\lambda P_{0}(t) $.

$ \dfrac{d P_{0}}{d t}=-\lambda P_{0} \Longrightarrow P_{0}(t)=e^{-\lambda t} $.

$ \dfrac{d P_{1}}{d t}+\lambda P_{1}=\lambda e^{-\lambda t} \Longrightarrow P_{1}(t)=\lambda t e^{-\lambda t} $

$ P_{k}(t)=\dfrac{(\lambda t)^{k} e^{-\lambda t}}{k!} $

## Definition 2

    
1. $N(0)=0$
2. $\{N(t), t \geq 0\}$ has independent increments
3. $ N(s, s+t] \sim  \operatorname{Poisson}(\lambda t), \forall s, t \geq 0 $.

## $T_n$: arrival interval 

- $T_1$ first arrival
- $T_n$: time between $n-1$ and $n$ arrivals

### Proposition

$T_n$ is i.i.d. Exponential RV with parameter $\lambda$.

#### Proof

$P(T_1 > t) = P(N(t) = 0) = e^{-\lambda t}, T_1 \sim \operatorname{Exp}(\lambda)$


### $S_n$: waiting interval 

$f_{S_n}(t) = \lambda e^{-\lambda t}\dfrac{(\lambda t)^{n-1}}{(n-1)!}$

####  Proof

$S_n = \sum_{i=1}^n T_i$

## Definition 3

Let $ \left\{T_{n}, n \geq 1\right\} $ be i.i.d. exponential random variables with mean $ 1 / \lambda $. Define $ S_{0}=0 $, $ S_{n}=\sum_{i=1}^{n} T_{i} $, and the counting process $ N(t)=\max \left\{n: S_{n} \leq t\right\} $. Then $ \{N(t), t \geq 0\} $ is a Poisson process with rate $ \lambda $.

### Proof

1. $ P(N(t)=n)=P\left(S_{n} \leq t\right)-P\left(S_{n+1} \leq t\right)=\int_{0}^{t} \frac{\lambda^{n} s^{n-1} e^{-\lambda s}}{(n-1)!} d s-\int_{0}^{t} \frac{\lambda^{n+1} s^{n} e^{-\lambda s}}{n!} d s =\frac{(\lambda t)^{n} e^{-\lambda t}}{n!} $,

2. For disjoint intervals $ \left(t_{i}, t_{j}\right] $, the increments $ N\left(t_{j}\right)-N\left(t_{i}\right) $ depend on disjoint subsets of $ \left\{T_{n}\right\} $. Since $ \left\{T_{n}\right\} $ are independent, the increments are independent.

3. The distribution of $ N(t+s)-N(s) $ depends only on $ t $. By the memoryless property, the process after time $ s $ is a replica of the original process, hence:$N(t+s)-N(s) \stackrel{d}{=} N(t)$

# Proposition 

## Thinning

Proposition 5.3 (Thinning of a Poisson Process). Let $ \{N(t), t \geq 0\} $ be a Poisson process with rate $ \lambda $. Suppose each event of $ \{N(t)\} $ is independently classified as a type- 1 event with probability $ p $ or a type-2 event with probability $ 1-p $. Define the counting processes:
$ N_{1}(t)= $ number of type-1 events by time $ t, \quad N_{2}(t)= $ number of type-2 events by time $ t $.
Then:
1. $ \left\{N_{1}(t), t \geq 0\right\} $ is a Poisson process with rate $ \lambda p $.
2. $ \left\{N_{2}(t), t \geq 0\right\} $ is a Poisson process with rate $ \lambda(1-p) $.
3. $ \left\{N_{1}(t)\right\} $ and $ \left\{N_{2}(t)\right\} $ are independent of each other.

### Proof 

$ P\left(N_{1}(t)=k\right)=\sum_{n=k}^{\infty} P(N(t)=n) \cdot P\left(N_{1}(t)=k \mid N(t)=n\right)= \sum_{n=k}^{\infty} \frac{(\lambda t)^{n} e^{-\lambda t}}{n!} \cdot\binom{n}{k} p^{k}(1-p)^{n-k} =\frac{(\lambda p t)^{k} e^{-\lambda p t}}{k!} \cdot \sum_{m=0}^{\infty} \frac{(\lambda(1-p) t)^{m} e^{-\lambda(1-p) t}}{m!}=\frac{(\lambda p t)^{k} e^{-\lambda p t}}{k!} $.

## Example 

You, as a business owner, want to sell a bubble tea machine. Offers arrive according to a Poisson process with rate $ \lambda $. Each offer is a random value with density function $ f(x) $. Once an offer is received, you must either accept it or reject it and wait for the next offer.
- While the machine remains unsold, you incur a holding cost of $ c $ per unit time. Your goal is to maximize the expected total return, defined as the selling price minus the total holding cost.
- Assume your strategy is to accept the first offer exceeding a threshold $ y $ (called a $ y $-strategy). What is the optimal value of $ y $ ?

### Solution

Step 1: Model Setup
- Offer Arrival: Offers follow a Poisson process with rate $ \lambda $.
- Offer Distribution: Each offer $ X $ has density $ f(x) $.
- Strategy: Accept the first offer $ X \geq y $.
- Holding Cost: $ c $ per unit time until sale.

Step 2: Key Quantities
1. Probability of Accepting an Offer:
$
p=P(X \geq y)=\int_{y}^{\infty} f(x) d x
$
2. Time to Sale: The first valid offer (i.e., $ X \geq y $ ) occurs at time $ T $, which follows an exponential distribution with rate $ \lambda p $. Thus:
$
E[T]=\frac{1}{\lambda p}
$
3. Expected Holding Cost:
$
E[\text { Holding Cost }]=c \cdot E[T]=\frac{c}{\lambda p}
$
4. Conditional Expected Selling Price:
$
E[X \mid X \geq y]=\frac{1}{p} \int_{y}^{\infty} x f(x) d x
$

Step 3: Expected Total Return
The expected total return $ V(y) $ is:
$
V(y)=E[\text { Selling Price }]-E[\text { Holding Cost }]=\frac{1}{p} \int_{y}^{\infty} x f(x) d x-\frac{c}{\lambda p}
$

Simplify:
$
V(y)=\frac{\int_{y}^{\infty} x f(x) d x-\frac{c}{\lambda}}{p} .
$

Step 4: Optimization
To maximize $ V(y) $, take the derivative $ V^{\prime}(y) $ and set it to zero:
$
V^{\prime}(y)=\frac{d}{d y}\left[\frac{\int_{y}^{\infty} x f(x) d x-\frac{c}{\lambda}}{\int_{y}^{\infty} f(x) d x}\right]=0
$

Derivative Calculation:
Let $ A(y)=\int_{y}^{\infty} x f(x) d x-\frac{c}{\lambda}, B(y)=\int_{y}^{\infty} f(x) d x $.
Using the quotient rule:
$
V^{\prime}(y)=\frac{A^{\prime}(y) B(y)-A(y) B^{\prime}(y)}{B(y)^{2}} .
$
- $ A^{\prime}(y)=-y f(y) $,
- $ B^{\prime}(y)=-f(y) $.

Substitute:
$
V^{\prime}(y)=\frac{(-y f(y)) B(y)-\left(\int_{y}^{\infty} x f(x) d x-\frac{c}{\lambda}\right)(-f(y))}{B(y)^{2}}
$

Simplify the numerator:
$
-f(y)\left[y B(y)-\int_{y}^{\infty} x f(x) d x+\frac{c}{\lambda}\right]=0
$

This implies:
$
y B(y)-\int_{y}^{\infty} x f(x) d x+\frac{c}{\lambda}=0
$

Step 5: Optimality Condition
Rearranging:
$
\int_{y}^{\infty}(x-y) f(x) d x=\frac{c}{\lambda}
$

Equivalently:
$
E\left[(X-y)^{+}\right]=\frac{c}{\lambda},
$
where $ (X-y)^{+}=\max (X-y, 0) $.

## Superposition

Let $ \left\{N_{1}(t), t \geq 0\right\},\left\{N_{2}(t), t \geq 0\right\}, \ldots $, be independent Poisson processes with rates $ \lambda_{1}, \lambda_{2}, \ldots $, respectively. Then, the combined process defined by
$
N(t)=N_{1}(t)+N_{2}(t)+\cdots
$
is a Poisson process with rate $ \lambda=\lambda_{1}+\lambda_{2}+\cdots $.

## Example 

Example: Let $ \left\{N_{1}(t), t \geq 0\right\} $ and $ \left\{N_{2}(t), t \geq 0\right\} $ be independent Poisson processes with rates $ \lambda_{1} $ and $ \lambda_{2} $, respectively. Let $ S_{n}^{1} $ denote the time of the $ n $-th event in $ \left\{N_{1}(t)\right\} $, and $ S_{m}^{2} $ denote the time of the $ m $-th event in $ \left\{N_{2}(t)\right\} $. Find $ P\left(S_{n}^{1}<S_{m}^{2}\right) $, i.e., the probability that the $ n $-th event of $ \left\{N_{1}(t)\right\} $ occurs before the $ m $-th event of $ \left\{N_{2}(t)\right\} $.

### Solution 

The number of events from $ \left\{N_{1}(t)\right\} $ among the first $ (n+m-1) $ merged events follows a binomial distribution with parameters $ (n+m-1, p) $. The desired probability is the sum of probabilities of having $ k \geq n $ successes (events from $ N_{1}(t) $ ):
$$
P\left(S_{n}^{1}<S_{m}^{2}\right)=\sum_{k=n}^{n+m-1}\binom{n+m-1}{k} p^{k}(1-p)^{n+m-1-k}
$$

## Order statistics

Let $ Y_{1}, \ldots, Y_{n} $ be independent and identically distributed (i.i.d.) random variables with probability density function $ f $. Let $ Y_{(k)} $ denote the $ k $-th smallest value among $ Y_{1}, \ldots, Y_{n} $. Then, $ Y_{(1)}, \ldots, Y_{(n)} $ are the order statistics of $ Y_{1}, \ldots, Y_{n} $.

The joint density of $ Y_{(1)}, \ldots, Y_{(n)} $ is given by:
$$
f_{Y_{(1)}, \ldots, Y_{(n)}}\left(y_{1}, \ldots, y_{n}\right)=n!\prod_{i=1}^{n} f\left(y_{i}\right), \quad \text { for } y_{1}<y_{2}<\cdots<y_{n} .
$$

### Theorem about uniform distribution 

Theorem: Let $ Y_{1}, Y_{2}, \ldots, Y_{n} $ be independent and identically distributed (i.i.d.) random variables uniformly distributed on the interval $ (0, t) $. Let $ Y_{(1)}, Y_{(2)}, \ldots, Y_{(n)} $ denote their order statistics. Then, the joint probability density function (PDF) of the order statistics is:
$$
f_{Y_{(1)}, Y_{(2)}, \ldots, Y_{(n)}}\left(y_{1}, y_{2}, \ldots, y_{n}\right)=\frac{n!}{t^{n}}, \quad \text { for } 0<y_{1}<y_{2}<\cdots<y_{n}<t
$$

### Theorem: poission process 

Let $ \{N(t), t \geq 0\} $ be a Poisson process with rate $ \lambda $. Given that $ N(t)=n $, the event times $ S_{1}, S_{2}, \ldots, S_{n} $ are the ordered times of the $ n $ events in $ (0, t) $. We aim to show that, conditioned on $ N(t)=n $, the joint distribution of $ S_{1}, S_{2}, \ldots, S_{n} $ is equivalent to the joint distribution of $ n $ independent uniform random variables on $ (0, t) $, sorted in ascending order.

### Theorem: More Thinning

Let $ N(t) $ be a Poisson process with rate $ \lambda $, and suppose each event occurring at time $ s $ is independently classified into type $ i(i=1, \ldots, k) $ with probability $ P_{i}(s) $. Let $ N_{i}(t) $ denote the number of type- $ i $ events by time $ t $. Then, the expected value of $ N_{i}(t) $ is:
$$
E\left[N_{i}(t)\right]=\lambda \int_{0}^{t} P_{i}(s) d s
$$



placeholder  


placeholder  


placeholder  


placeholder  


